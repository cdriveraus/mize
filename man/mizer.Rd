% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/mizer.R
\name{mizer}
\alias{mizer}
\title{Numerical Optimization}
\usage{
mizer(par, fg, method = "L-BFGS", norm_direction = FALSE, memory = 10,
  scale_hess = TRUE, cg_update = "PR+", nest_q = 0,
  nest_convex_approx = FALSE, nest_burn_in = 0, use_nest_mu_zero = FALSE,
  kappa = 1.1, kappa_fun = "*", phi = 0.5, theta = 0.1,
  line_search = "MT", c1 = 1e-04, c2 = NULL, step0 = NULL,
  ls_initializer = NULL, try_newton_step = NULL, mom_type = NULL,
  mom_schedule = NULL, mom_init = NULL, mom_final = NULL,
  mom_switch_iter = NULL, mom_linear_weight = FALSE, restart = NULL,
  max_iter = 100, max_fn = Inf, max_gr = Inf, max_fg = Inf,
  abs_tol = sqrt(.Machine$double.eps), rel_tol = abs_tol, grad_tol = NULL,
  check_conv_every = 1, verbose = FALSE, store_progress = FALSE)
}
\arguments{
\item{par}{Initial values for the function to be optimized over.}

\item{fg}{Function and gradient list. See 'Details'.}

\item{method}{Optimization method. See 'Details'.}

\item{norm_direction}{If \code{TRUE}, then the steepest descent direction
is normalized to unit length. Useful for adaptive step size methods where
the previous step size is used to initialize the next iteration.}

\item{memory}{The number of updates to store if using the \code{L-BFGS}
method. Ignored otherwise. Must be a positive integer.}

\item{scale_hess}{if \code{TRUE}, the approximation to the inverse Hessian
is scaled according to the method described by Nocedal and Wright
(approximating an eigenvalue). Applies only to the methods \code{BFGS}
(where the scaling is applied only during the first step) and \code{L-BFGS}
(where the scaling is applied during every iteration). Ignored otherwise.}

\item{cg_update}{Type of update to use for the \code{CG} method. Can be
one of \code{"FR"} (Fletcher-Reeves), \code{"PR"} (Polak-Ribiere),
\code{"PR+"} (Polak-Ribiere with a reset to steepest descent), \code{"HS"}
(Hestenes-Stiefel), or \code{"DY"} (Dai-Yuan). Ignored if \code{method} is
not \code{"CG"}.}

\item{nest_q}{Strong convexity parameter for the NAG
momentum term. Must take a value between 0 (strongly convex) and 1
(zero momentum). Only applies using the NAG method or a momentum method with
Nesterov momentum schedule. Also does nothing if \code{nest_convex_approx}
is \code{TRUE}.}

\item{nest_convex_approx}{If \code{TRUE}, then use an approximation due to
Sutskever for calculating the momentum parameter in the NAG method. Only
applies using the NAG method or a momentum method with Nesterov momentum
schedule.}

\item{nest_burn_in}{Number of iterations to wait before using a non-zero
momentum. Only applies using the NAG method or a momentum method with
Nesterov momentum schedule.}

\item{use_nest_mu_zero}{If \code{TRUE}, then the momentum on iteration zero
is set to 0.4. Otherwise, it's zero. Only applies using the NAG method or a
momentum method with Nesterov momentum schedule.}

\item{kappa}{Value by which to increase the step size for the \code{"bold"}
step size method or the \code{"DBD"} method.}

\item{kappa_fun}{Operator to use when combining the current step size with
\code{kappa}. Can be one of \code{"*"} (to multiply the current step size
with \code{kappa}) or \code{"+"} (to add).}

\item{phi}{Multiplier to reduce the step size by if using the \code{"DBD"}
method or the \code{"bold"} or \code{"back"} line search method. Should be
a positive value less than 1.}

\item{theta}{Weighting parameter used by the \code{"DBD"} method only, and
only if no momentum scheme is provided. Must be an integer between 0 and 1.}

\item{line_search}{Type of line search to use. See 'Details'.}

\item{c1}{Sufficient decrease parameter for Wolfe-type line searches. Should
be a value between 0 and 1.}

\item{c2}{Sufficient curvature parameter for line search for Wolfe-type line
searches. Should be a value between \code{c1} and 1.}

\item{step0}{Initial value for the line search on the first step. See
'Details'.}

\item{ls_initializer}{For Wolfe-type line searches only, how to initialize
the line search on iterations after the first. See 'Details'.}

\item{try_newton_step}{For Wolfe-type line searches only, try the
line step value of 1 as the initial step size whenever \code{ls_initializer}
suggests a step size > 1. Defaults to \code{TRUE} for quasi-Newton methods
such as BFGS and L-BFGS, \code{FALSE} otherwise.}

\item{mom_type}{Momentum type, either \code{"classical"} or
\code{"nesterov"}. See 'Details'.}

\item{mom_schedule}{Momentum schedule. See 'Details'.}

\item{mom_init}{Initial momentum value.}

\item{mom_final}{Final momentum value.}

\item{mom_switch_iter}{For \code{mom_schedule} \code{"switch"} only, the
iteration when \code{mom_init} is changed to \code{mom_final}.}

\item{mom_linear_weight}{If \code{TRUE}, the gradient contribution to the
update is weighted using momentum contribution.}

\item{restart}{Momentum restart type. Can be one of "fn" or "gr". See
'Details'. Ignored if no momentum scheme is being used.}

\item{max_iter}{Maximum number of iterations to optimize for. Defaults to
100. See the 'Convergence' section for details.}

\item{max_fn}{Maximum number of function evaluations. See the 'Convergence'
section for details.}

\item{max_gr}{Maximum number of gradient evaluations. See the 'Convergence'
section for details.}

\item{max_fg}{Maximum number of function or gradient evaluations. See the
'Convergence' section for details.}

\item{abs_tol}{Absolute tolerance for comparing two function evaluations.
See the 'Convergence' section for details.}

\item{rel_tol}{Relative tolerance for comparing two function evaluations.
See the 'Convergence' section for details.}

\item{grad_tol}{Absolute tolerance for the length (l2-norm) of the gradient
vector. See the 'Convergence' section for details.}

\item{check_conv_every}{Positive integer indicating how often to check
convergence. Default is 1, i.e. every iteration. See the 'Convergence'
section for details.}

\item{verbose}{If \code{TRUE}, log information about the progress of the
optimization to the console.}

\item{store_progress}{If \code{TRUE} store information about the progress
of the optimization in a data frame, and include it as part of the return
value.}
}
\value{
A list with components:
\itemize{
 \item{\code{par}} Optimized parameters. Normally, this is the best set of
 parameters seen during optimization, i.e. the set that produced the minimum
 function value. This requires that convergence checking with is carried out,
 including function evaluation where necessary. See the 'Convergence'
 section for details.
 \item{\code{nf}} Total number of function evaluations carried out. This
 includes any extra evaluations required for convergence calculations. Also,
 a function evaluation may be required to calculate the value of \code{f}
 returned in this list (see below). Additionally, if the \code{verbose}
 parameter is \code{TRUE}, then function and gradient information for the
 initial value of \code{par} will be logged to the console. These values
 are cached for subsequent use by the optimizer.
 \item{\code{ng}} Total number of gradient evaluations carried out. This
 includes any extra evaluations required for convergence calculations using
 \code{grad_tol}. As with \code{nf}, additional gradient calculations beyond
 what you're expecting may have been needed for logging, convergence and
 calculating the value of \code{g2n} (see below).
 \item{\code{f}} Value of the function, evaluated at the returned
 value of \code{par}.
 \item{\code{g2n}} Optional: the length (Euclidean or l2-norm) of the
 gradient vector, evaluated at the returned value of \code{par}. Calculated
 only if \code{grad_tol} is non-null.
 \item{\code{iter}} The number of iterations the optimization was carried
 out for.
 \item{\code{terminate}} List containing items: \code{what}, indicating what
 convergence criterion was met, and \code{val} specifying the value at
 convergence. See the 'Convergence' section for more details.
 \item{\code{progress}} Optional data frame containing information on the
 value of the function, gradient, momentum, and step sizes evaluated at each
 iteration where convergence is checked. Only present if
 \code{store_progress} is set to \code{TRUE}. Could get quite large if the
 optimization is long and the convergence is checked regularly.
}
}
\description{
Numerical optimization including conjugate gradient,
Broyden-Fletcher-Goldfarb-Shanno (BFGS), and the limited memory BFGS.
}
\details{
The function to be optimized should be passed as a list to the \code{fg}
parameter. This should consist of:
\itemize{
\item{\code{fn}}. The function to be optimized. Takes a vector of parameters
  and returns a scalar.
\item{\code{gr}}. The gradient of the function. Takes a vector of parameters
and returns a vector with the same length as the input parameter vector.
\item{\code{fg}}. Optional function which calculates the function and
gradient in thesame routine. Takes a vector of parameters and returns a list
containing the function result as \code{fn} and the gradient result as
\code{gr}.
}

The \code{fg} function is optional, but for some methods (e.g. line search
methods based on the Wolfe criteria), both the function and gradient values
are needed for the same parameter value. Calculating them in the same
function can save time if there is a lot of shared work.
}
\section{Optimization Methods}{

The \code{method} specifies the optimization method:

\itemize{
\item \code{"SD"} is plain steepest descent. Not very effective on its own,
but can be combined with various momentum approaches.
\item \code{"BFGS"} is the Broyden-Fletcher-Goldfarb-Shanno quasi-Newton
method. This stores an approximation to the inverse of the Hessian of the
function being minimized, which requires storage proportional to the
square of the length of \code{par}, so is unsuitable for large problems.
\item \code{"L-BFGS"} is the Limited memory Broyden-Fletcher-Goldfarb-Shanno
quasi-Newton method. This does not store the inverse Hessian approximation
directly and so can scale to larger-sized problems than \code{"BFGS"}. The
amount of memory used can be controlled with the \code{memory} parameter.
\item \code{"CG"} is the conjugate gradient method. The \code{cg_update}
parameter allows for different methods for choosing the next direction:
  \itemize{
    \item \code{"FR"} The method of Fletcher and Reeves.
    \item \code{"PR"} The method of Polak and Ribiere.
    \item \code{"PR+"} The method of Polak and Ribiere with a restart to
    steepest descent if conjugacy is lost. The default.
    \item \code{"HS"} The method of Hestenes and Stiefel.
    \item \code{"DY"} The method of Dai and Yuan.
  }
\item \code{"NAG"} is the Nesterov Accelerated Gradient method. The exact
form of the momentum update in this method can be controlled with the
following parameters:
  \itemize{
  \item{\code{nest_q}} Strong convexity parameter. Must take a value
  between 0 (strongly convex) and 1 (zero momentum). Ignored if
  \code{nest_convex_approx} is \code{TRUE}.
  \item{\code{nest_convex_approx}} If \code{TRUE}, then use an approximation
  due to Sutskever for calculating the momentum parameter.
  \item{\code{use_nest_mu_zero}} If \code{TRUE}, then the momentum on
  iteration zero is set to 0.4. Otherwise, it's zero. Ignored if
  \code{nest_convex_approx} is \code{FALSE}.
  \item{\code{nest_burn_in}} Number of iterations to wait before using a
  non-zero momentum.
  }
\item \code{"DBD"} is the Delta-Bar-Delta method of Jacobs.
\item \code{"MOM"} is steepest descent with momentum. See below for momentum
options.
}

For more details on gradient-based optimization in general, and the BFGS,
L-BFGS and CG methods, see Nocedal and Wright.
}

\section{Line Search}{

The parameter \code{line_search} determines the line search to be carried
out:

\itemize{
  \item If a numeric scalar is provided, then a constant value will be used
  for the line search. Note that this value will be multiplied by the
  magnitude of the direction vector used in the gradient descent method.
  For method \code{"SD"} only, setting the \code{norm_direction} parameter to
  \code{TRUE} will scale the direction vector so it has unit length.
  \item \code{"RAS"} carries out a line search using the strong Wolfe
  conditions and the method of Rasmussen.
  \item \code{"MT"} carries out a line search using the strong Wolfe
  conditions and the method of More-Thuente.
  \item \code{"BOLD"} carries out a back tracking line search until a
  reduction in the function value is found.
}

If using one of the methods: \code{"BFGS"}, \code{"L-BFGS"}, \code{"CG"} or
\code{"NAG"}, one of the Wolfe line searches, \code{"RAS"} or \code{"MT"}
should be used, otherwise very poor performance is likely to be encountered.
The following parameters can be used to control the line search:

 \itemize{
   \item{\code{c1}} The sufficient decrease condition. Normally left at its
   default value of 1e-4.
   \item{\code{c2}} The sufficient curvature condition. Defaults to 0.9 if
   using the methods \code{"BFGS"} and \code{"L-BFGS"}, and to 0.1 for
   every other method, more or less in line with the recommendations given
   by Nocedal and Wright. The smaller the value of \code{c2}, the stricter
   the line search, but it should not be set to smaller than \code{c1}.
   \item{\code{step0}} Initial value for the line search on the first step.
   If a positive numeric value is passed as an argument, that value is used
   as-is. Otherwise, by passing a character as an argument, a guess is made
   based on the gradient at the starting point:
   \itemize{
     \item{\code{"r"}} As used by Rasmussen in \code{minimize.m}:
     \deqn{\frac{1}{1+\left|g\right|^2}}{1 / 1 + (|g|^2)}
     \item{\code{"s"}} As used in scipy's \code{optimize.py}
     \deqn{\frac{1}{\left|g\right|}}{1 / |g|}
     \item{\code{"m"}} As used by Schmidt in \code{minFunc.m}
     (the reciprocal of the l1 norm of g)
     \deqn{\frac{1}{\left|g\right|_1}}{1 / |g|1}
   }
   \item{\code{ls_initializer}} How to initialize subsequent line searches
   after the first, using results from the previous line search,
   based on two suggestions mentioned by Nocedal and Wright:
   \itemize{
     \item{\code{"r"}} Slope ratio method.
     \item{\code{"q"}} Quadratic interpolation method.
   }
   \item{\code{try_newton_step}} For quasi-Newton methods (\code{"BFGS"} and
   \code{"L-BFGS"}), setting this to \code{TRUE} will try the "natural" step
   size of 1, whenever the \code{ls_initializer} method suggests an initial
   step size larger than that. On by default for BFGS and L-BFGS, off for
   everything else.
}

If the \code{"DBD"} is used for the optimization \code{"method"}, then the
\code{line_search} parameter is ignored, because this method controls both
the direction of the search and the step size simultaneously. The following
parameters can be used to control the step size:

\itemize{
  \item{\code{kappa}} The amount by which to increase the step size in a
  direction where the current step size is deemed to be too short. This
  should be a positive scalar.
  \item{\code{phi}} The amount by which to decrease the step size in a
  direction where the currents step size is deemed to be too long. This
  should be a positive scalar smaller than 1.
  \item{\code{kappa_fun}} How to increase the step size: either the method of
  Jacobs (addition of \code{kappa}) or Janet and co-workers (multiplication
  by \code{kappa}). Note that the step size decrease \code{phi} is always
  a multiplication.
}

The \code{"BOLD"} line search also uses the \code{kappa} and \code{phi}
parameters with similar meanings to their use with the \code{"DBD"} method:
the backtracking portion reduces the step size by a factor of \code{phi}.
Once a satisfactory step size has been found, the line search for the
next iteration is initialized by multiplying the previously found step size
by \code{kappa}.
}

\section{Momentum}{

For \code{method} \code{"MOM"}, momentum schemes can be accessed through the
momentum arguments:

\itemize{
\item{\code{mom_type}} Momentum type, either \code{"classical"} or
 \code{"nesterov"}. Using "Nesterov" applies the momentum step before the
  gradient descent as suggested by Sutskever, emulating the behavior of the
  Nesterov Accelerated Gradient method.
\item{\code{mom_schedule}} How the momentum changes over the course of the
  optimization:
  \itemize{
  \item{If a numerical scalar is provided, a constant momentum will be
    applied throughout.}
  \item{\code{"nesterov"}} Use the momentum schedule from the Nesterov
  Accelerated Gradient method. Parameters which control the NAG momentum
  can also be used in combination with this option.
  \item{\code{"switch"}} Switch from one momentum value (specified via
  \code{mom_init}) to another (\code{mom_final}) at a
  a specified iteration (\code{mom_switch_iter}).
  \item{\code{"ramp"}} Linearly increase from one momentum value
  (\code{mom_init}) to another (\code{mom_final}) over the specified
  period (\code{max_iter}).
  }
}

The \code{restart} parameter provides a way to restart the momentum if the
optimization appears to be not be making progress, using the method of
O'Donoghue and Candes. There are two strategies:
\itemize{
  \item{\code{"fn"}} A restart is applied if the function does not decrease
  on consecutive iterations.
  \item{\code{"gr"}} A restart is applied if the direction of the
  optimization is not a descent direction.
}

The effect of the restart is to "forget" any previous momentum update vector,
and, for those momentum schemes that change with iteration number, to
effectively reset the iteration number back to zero. If the \code{mom_type}
is \code{"nesterov"}, the gradient-based restart is not available.

If \code{method} type \code{"MOM"} is specified with no other values, the
momentum scheme will default to a constant value of \code{0.9}, with a
function-based restart.
}

\section{Convergence}{


There are several ways for the optimization to terminate. The type of
termination is communicated by a two-item list \code{terminate} in the return
value, consisting of \code{what}, a short string describing what caused the
termination, and \code{val}, the value of the termination criterion that
causes termination.

The following parameters control various stopping criteria:

\itemize{
  \item{\code{max_iter}} Maximum number of iterations to calculate. Reaching
  this limit is indicated by \code{terminate$what} being \code{"max_iter"}.
  \item{\code{max_fn}} Maximum number of function evaluations allowed.
  Indicated by \code{terminate$what} being \code{"max_fn"}.
  \item{\code{max_gr}} Maximum number of gradient evaluations allowed.
  Indicated by \code{terminate$what} being \code{"max_gr"}.
  \item{\code{max_fg}} Maximum number of gradient evaluations allowed.
  Indicated by \code{terminate$what} being \code{"max_fg"}.
  \item{\code{abs_tol}} Absolute tolerance of the function value. If the
  absolute value of the function falls below this threshold,
  \code{terminate$what} will be \code{"abs_tol"}. Will only be triggered if
  the objective function has a minimum value of zero.
  \item{\code{rel_tol}} Relative tolerance of the function value, comparing
  consecutive function evaluation results. Indicated by \code{terminate$what}
  being \code{"rel_tol"}.
  \item{\code{grad_tol}} Absolute tolerance of the l2 (Euclidean) norm of
  the gradient. Indicated by \code{terminate$what} being \code{"grad_tol"}.
  Note that the gradient norm is not a very reliable stopping criterion
  (see Nocedal and co-workers 2002), but is quite  commonly used, so this
  might be useful for comparison with results from other optimizers.
}

Convergence is checked between specific interations. How often is determined
by the \code{check_conv_every} parameter, which specifies the number of
iterations between each check. By default, this is set for every iteration.

Be aware that if \code{abs_tol} or \code{rel_tol} are non-\code{NULL}, this
requires the function to have been evaluated at the current position at the
end of each iteration. If the function at that  position hasn't been
calculated, it will be calculated and will contribute to the total reported
in the \code{counts} list in the return value. The calculated function value
is cached for use by the optimizer in the next iteration, so if the optimizer
would have needed to calculate the function anyway (e.g. use of the strong
Wolfe line search methods), there is no significant cost accrued by
calculating it earlier for convergence calculations. However, for methods
that don't use the function value at that location, this could represent a
lot of extra function evaluations. On the other hand, not checking
convergence could result in a lot of extra unnecessary iterations.
Similarly, if \code{grad_tol} is non-\code{NULL}, then the gradient will
be calculated if needed.

If extra function or gradient evaluations is an issue, set
\code{check_conv_every} to a higher value, but be aware that this can cause
convergence limits to be exceeded by a greater amount.

Note also that if the \code{verbose} parameter is \code{TRUE}, then a summary
of the results so far will be logged to the console whenever a convergence
check is carried out. If the \code{store_progress} parameter is \code{TRUE},
then the same information will be returned as a data frame in the return
value. For a long optimization this could be a lot of data, so by default it
is not stored.

Other ways for the optimization to terminate is if an iteration generates a
non-finite (i.e. \code{Inf} or \code{NaN}) gradient or function value.
Some, but not all, line-searches will try to recover from the latter, by
reducing the step size, but a non-finite gradient calculation during the
gradient descent portion of opimization is considered catastrophic by mizer,
and it will give up. Termination under non-finite gradient or function
conditions will result in \code{terminate$what} being \code{"gr_inf"} or
\code{"fn_inf"} respectively. Unlike the convergence criteria, the
optimization will detect these error conditions and terminate even if a
convergence check would not be carried out for this iteration.

The value of \code{par} in the return value should be the parameters which
correspond to the lowest value of the function that has been calculated
during the optimization. As discussed above however, determining which set
of parameters requires a function evaluation at the end of each iteration,
which only happens if either the optimization method calculates it as part
of its own operation or if a convergence check is being carried out during
this iteration. Therefore, if your method doesn't carry out function
evaluations and \code{check_conv_every} is set to be so large that no
convergence calculation is carried out before \code{max_iter} is reached,
then the returned value of \code{par} is the last value encountered.
}
\examples{
# Function to optimize and starting point defined after creating optimizer
rosenbrock_fg <- list(
  fn = function(x) { 100 * (x[2] - x[1] * x[1]) ^ 2 + (1 - x[1]) ^ 2  },
  gr = function(x) { c( -400 * x[1] * (x[2] - x[1] * x[1]) - 2 * (1 - x[1]),
                         200 *        (x[2] - x[1] * x[1])) })
rb0 <- c(-1.2, 1)

# Minimize using L-BFGS
res <- mizer(rb0, rosenbrock_fg, method = "L-BFGS")

# Conjugate gradient with Fletcher-Reeves update, tight Wolfe line search
res <- mizer(rb0, rosenbrock_fg, method = "CG", cg_update = "FR", c2 = 0.1)

# Steepest decent with constant momentum = 0.9
res <- mizer(rb0, rosenbrock_fg, method = "SD", mom_type = "classical",
             mom_schedule = 0.9)

# Steepest descent with constant momentum in the Nesterov style as described
# by Sutskever and co-workers
res <- mizer(rb0, rosenbrock_fg, method = "SD", mom_type = "nesterov",
             mom_schedule = 0.9)

# Nesterov momentum with adaptive restart comparing function values
res <- mizer(rb0, rosenbrock_fg, method = "SD", mom_type = "nesterov",
             mom_schedule = 0.9, restart = "fn")
}
\references{
Jacobs, R. A. (1988).
Increased rates of convergence through learning rate adaptation.
\emph{Neural networks}, \emph{1}(4), 295-307.

Janet, J. A., Scoggins, S. M., Schultz, S. M., Snyder, W. E., White, M. W.,
& Sutton, J. C. (1998, May).
Shocking: An approach to stabilize backprop training with greedy adaptive
learning rates.
In \emph{1998 IEEE International Joint Conference on Neural Networks Proceedings.}
(Vol. 3, pp. 2218-2223). IEEE.

Nocedal, J., Sartenaer, A., & Zhu, C. (2002).
On the behavior of the gradient norm in the steepest descent method.
\emph{Computational Optimization and Applications}, \emph{22}(1), 5-35.

Nocedal, J., & Wright, S. (2006).
Numerical optimization.
Springer Science & Business Media.

O'Donoghue, B., & Candes, E. (2013).
Adaptive restart for accelerated gradient schemes.
\emph{Foundations of computational mathematics}, \emph{15}(3), 715-732.

Sutskever, I., Martens, J., Dahl, G., & Hinton, G. (2013).
On the importance of initialization and momentum in deep learning.
In \emph{Proceedings of the 30th international conference on machine learning (ICML-13)}
(pp. 1139-1147).
}

